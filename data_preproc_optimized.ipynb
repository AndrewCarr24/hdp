{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ecoronado/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ecoronado/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cppimport\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer as stemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.stem\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "stemmer = stemmer(\"english\")\n",
    "\n",
    "## Import C++ functions\n",
    "preproc_cpp = cppimport.imp(\"hdp_preproc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    '''Function that lemmatizes words in abstract by verbs'''\n",
    "    \n",
    "    return [stemmer.stem(WordNetLemmatizer().lemmatize(w, pos='v')) \n",
    "            for w in preproc_cpp.text_cleanup(doc[0])]\n",
    "\n",
    "def full_preprocess(doc, st_words):\n",
    "    '''Performs word lemmatization and stopword removal'''\n",
    "    return preproc_cpp.rm_stops_n_shorts(preprocess(doc), st_words, 3)\n",
    "\n",
    "\n",
    "def tf(in_docs):\n",
    "    '''Term frequency matrix function, calculates the term frequencies of word from an \n",
    "       a list of documents text. Then filtered according to frequency criteria to keep shared \n",
    "       yet low occurence words.\n",
    "       The output is the filtered term frequency table and associated vocabulary'''\n",
    "    \n",
    "    v = preproc_cpp.generate_vocab(in_docs) # generates vocab\n",
    "    tf = preproc_cpp.tf_cpp(in_docs, v) # generates tf matrix\n",
    "    \n",
    "    filt_df = preproc_cpp.filter_tf_cpp(tf) # filters tf matrix (last column is vocab indexes)\n",
    "    \n",
    "    # Filter vocab indexes\n",
    "    v_idx = filt_df[:, filt_df.shape[1]-1].astype(int) # get vocab indexes\n",
    "    vocab = np.array(v)[v_idx]\n",
    "    \n",
    "    return vocab, filt_df[:,1:filt_df.shape[1]]\n",
    "\n",
    "\n",
    "\n",
    "def get_docs(df):\n",
    "    '''Get list of sublists (len = documents), with each sublist containing unique word ids per document'''\n",
    "    \n",
    "    return preproc_cpp.get_docs(df)\n",
    "\n",
    "    \n",
    "def data_preproc(file_path):\n",
    "    '''Data pre-processing function\n",
    "       Input -> url to data in CSV format where each row is a document text'''\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "        \n",
    "    st_words = set(stopwords.words('english'))\n",
    "    \n",
    "    in_docs = [full_preprocess(d, st_words) for d in df.values]\n",
    "    \n",
    "    vocab, filtered_df = tf(in_docs)\n",
    "            \n",
    "    docs = get_docs(filtered_df)\n",
    "    \n",
    "    return vocab, docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeIt profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tm_test_data.csv\")\n",
    "    \n",
    "in_docs = {k: str(txt[0]) for k,txt in enumerate(df.values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -r2  data_preproc(\"tm_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, docs = data_preproc(\"tm_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[59,\n",
       " 104,\n",
       " 269,\n",
       " 380,\n",
       " 388,\n",
       " 396,\n",
       " 488,\n",
       " 547,\n",
       " 592,\n",
       " 617,\n",
       " 637,\n",
       " 675,\n",
       " 729,\n",
       " 811,\n",
       " 854,\n",
       " 957,\n",
       " 978,\n",
       " 988,\n",
       " 1045,\n",
       " 1111,\n",
       " 1275,\n",
       " 1309,\n",
       " 1321,\n",
       " 1487,\n",
       " 1501,\n",
       " 1544]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prun profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 10 percent of what takes longest\n",
    "%prun -l 0.05 -s cumtime data_preproc(\"tm_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cppimport\n",
    "\n",
    "\n",
    "preproc_cpp = cppimport.imp(\"hdp_preproc\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tm_test_data.csv\")\n",
    "    \n",
    "#in_docs = {k: str(txt[0]) for k,txt in enumerate(df.values)}\n",
    "st_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [stemmer.stem(WordNetLemmatizer().lemmatize(w, pos='v')) \n",
    "            for w in preproc_cpp.text_cleanup(df.values[0][0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_cpp.rm_stops_n_shorts(test, st_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_docs = [full_preprocess(d, st_words) for d in df.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = preproc_cpp.generate_vocab(in_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = preproc_cpp.tf_cpp(in_docs, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4714, 622)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_df = preproc_cpp.filter_tf_cpp(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1555, 623)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filt_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(v)[filt_df[:,622].astype(int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = preproc_cpp.get_docs(filt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['algorithm', 'analyz', 'avail', 'brief', 'code', 'comparison',\n",
       "       'current', 'dataset', 'demonstr', 'dialog', 'discuss', 'featur',\n",
       "       'final', 'high', 'homogen', 'interact', 'introduc', 'introduct',\n",
       "       'larg', 'lisp', 'main', 'make', 'miss', 'modif', 'object', 'order',\n",
       "       'orient', 'overview', 'paper', 'perform', 'plot', 'presenc',\n",
       "       'present', 'produc', 'program', 'small', 'techniqu', 'userfriend',\n",
       "       'well'], dtype='<U38')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[test[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
