{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ecoronado/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ecoronado/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cython\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer as stemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.stem\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "stemmer = stemmer(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    '''Function that lemmatizes words in abstract by verbs'''\n",
    "    \n",
    "    return [stemmer.stem(WordNetLemmatizer().lemmatize(w, pos='v')) \n",
    "            for w in doc.translate(str.maketrans('','', string.punctuation)).lower().split(' ')]\n",
    "\n",
    "\n",
    "def rm_stopwords_and_short_words(words, st_words):\n",
    "    '''Function removes stop words and those with length < 3'''\n",
    "    results = []\n",
    "    for i in words:\n",
    "        if not i in st_words and len(i)  > 3:\n",
    "            results.append(i)\n",
    "    return results\n",
    "\n",
    "def full_preprocess(doc, st_words):\n",
    "    '''Performs word lemmatization and stopword removal'''\n",
    "    return rm_stopwords_and_short_words(preprocess(doc), st_words)\n",
    "\n",
    "\n",
    "def tf(docs, st_words):\n",
    "    '''Term frequency matrix function, calculates the term frequencies of word from an text-document paired dictionary input. \n",
    "       The output is a term frequency table '''\n",
    "    \n",
    "    # generate counts per document\n",
    "    counts = {k: Counter(full_preprocess(txt, st_words)) for k, txt in docs.items()}\n",
    "    tf_df = pd.DataFrame.from_dict(counts).fillna(0).astype(int) # build pandas df, fill empty vals with 0s\n",
    "    \n",
    "    return(tf_df)\n",
    "\n",
    "\n",
    "def token_filtering(tf_df):\n",
    "    '''Filters out tokens that appear in fewer than 3 abstracts and tokens that appear in more than half the abstracts '''\n",
    "    filtered_df = tf_df[(tf_df.sum(axis=1) > 3)]\n",
    "    filtered_df = filtered_df[(filtered_df.astype(bool).sum(axis=1) / tf_df.shape[1] < 0.5)]\n",
    "    \n",
    "    return filtered_df\n",
    "    \n",
    "def get_docs(df):\n",
    "    '''quickly build a dictionary based on filtered dataframe, get words w/ unique ids'''\n",
    "    df.reset_index(inplace=True)\n",
    "    filt_words = pd.DataFrame.to_dict(df.drop(columns='index'))\n",
    "    \n",
    "    return [[word for word, cnt in words.items() if cnt!=0] for dkeys, words in filt_words.items()]\n",
    "    \n",
    "    \n",
    "def data_preproc(file_path):\n",
    "    '''Data pre-processing function\n",
    "       Input -> url to data in CSV format where each row is a document text'''\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    in_docs = {k: str(txt[0]) for k,txt in enumerate(df.values)}\n",
    "    \n",
    "    st_words = stopwords.words('english')\n",
    "    \n",
    "    tf_df = tf(in_docs, st_words)\n",
    "    \n",
    "    filtered_df = token_filtering(tf_df)\n",
    "    \n",
    "    vocab = filtered_df.index.values\n",
    "        \n",
    "    docs = get_docs(filtered_df)\n",
    "    \n",
    "    return [vocab, docs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeIt profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.37 s ± 58.5 ms per loop (mean ± std. dev. of 2 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r2  data_preproc(\"tm_test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prun profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         7090720 function calls (7089987 primitive calls) in 3.180 seconds\n",
       "\n",
       "   Ordered by: cumulative time\n",
       "   List reduced from 528 to 26 due to restriction <0.05>\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "        1    0.000    0.000    3.180    3.180 {built-in method builtins.exec}\n",
       "        1    0.002    0.002    3.180    3.180 <string>:1(<module>)\n",
       "        1    0.001    0.001    3.178    3.178 <ipython-input-6-3c5676dfe741>:47(data_preproc)\n",
       "        1    0.003    0.003    3.148    3.148 <ipython-input-6-3c5676dfe741>:21(tf)\n",
       "        1    0.002    0.002    2.645    2.645 <ipython-input-6-3c5676dfe741>:26(<dictcomp>)\n",
       "      622    0.002    0.000    2.630    0.004 <ipython-input-6-3c5676dfe741>:16(full_preprocess)\n",
       "      622    0.002    0.000    2.452    0.004 <ipython-input-6-3c5676dfe741>:1(preprocess)\n",
       "      622    0.097    0.000    2.435    0.004 <ipython-input-6-3c5676dfe741>:4(<listcomp>)\n",
       "    82463    0.912    0.000    1.770    0.000 snowball.py:1406(stem)\n",
       "  5119360    0.732    0.000    0.732    0.000 {method 'endswith' of 'str' objects}\n",
       "    82463    0.056    0.000    0.559    0.000 wordnet.py:40(lemmatize)\n",
       "    82463    0.104    0.000    0.487    0.000 wordnet.py:1873(_morphy)\n",
       "        7    0.000    0.000    0.480    0.069 frame.py:414(__init__)\n",
       "        2    0.006    0.003    0.479    0.240 construction.py:213(init_dict)\n",
       "        1    0.000    0.000    0.476    0.476 frame.py:1168(from_dict)\n",
       "        2    0.000    0.000    0.467    0.233 construction.py:56(arrays_to_mgr)\n",
       "        2    0.005    0.003    0.439    0.220 construction.py:300(_homogenize)\n",
       "      622    0.402    0.001    0.408    0.001 {pandas._libs.lib.fast_multiget}\n",
       "    87209    0.035    0.000    0.281    0.000 wordnet.py:1884(apply_rules)\n",
       "    87209    0.134    0.000    0.246    0.000 wordnet.py:1886(<listcomp>)\n",
       "      622    0.169    0.000    0.176    0.000 <ipython-input-6-3c5676dfe741>:8(rm_stopwords_and_short_words)\n",
       "    65286    0.108    0.000    0.118    0.000 snowball.py:231(_r1r2_standard)\n",
       "    90424    0.095    0.000    0.102    0.000 wordnet.py:1892(filter_forms)\n",
       "501759/501044    0.038    0.000    0.039    0.000 {built-in method builtins.len}\n",
       "   197123    0.038    0.000    0.038    0.000 {method 'startswith' of 'str' objects}\n",
       "   262220    0.038    0.000    0.038    0.000 {method 'replace' of 'str' objects}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get top 10 percent of what takes longest\n",
    "%prun -l 0.05 -s cumtime data_preproc(\"tm_test_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
