abstract
"In this paper a highly interactive, user-friendly Lisp program is introduced to perform homogeneity analysis. A brief introduction to the technique is presented as well as its modification in the presence of missing data. The algorithm and its Lisp implemenation is discussed, and an overview of the object oriented code that produces the interactive dialogs and plots is provided. In order to demonstrate the main features of the program, a small and a large dataset are analyzed. Finally, some comparisons are made with other currently available programs."
"The fit of a variogram model to spatially-distributed data is often difficult to assess. A graphical diagnostic written in S-plus is introduced that allows the user to determine both the general quality of the fit of a variogram model, and to find specific pairs of locations that do not have measurements that are consonant with the fitted variogram. It can help identify nonstationarity, outliers, and poor variogram fit in general. Simulated data sets and a set of soil nitrogen concentration data are examined using this graphical diagnostic."
"This paper describes the incorporation of seven stand-alone clustering programs into S-PLUS, where they can now be used in a much more flexible way. The original Fortran programs carried out new cluster analysis algorithms introduced in the book of Kaufman and Rousseeuw (1990). These clustering methods were designed to be robust and to accept dissimilarity data as well as objects-by-variables data. Moreover, they each provide a graphical display and a quality index reflecting the strength of the clustering. The powerful graphics of S-PLUS made it possible to improve these graphical representations considerably.  The integration of the clustering algorithms was performed according to the object-oriented principle supported by S-PLUS. The new functions have a uniform interface, and are compatible with existing S-PLUS functions. We will describe the basic idea and the use of each clustering method, together with its graphical features. Each function is briefly illustrated with an example."
"The Java programming language has added a new tool for delivering computing applications over the World Wide Web (WWW). WebStat is a new computing environment for basic statistical analysis which is delivered in the form of a Java applet. Anyone with WWW access and a Java capable browser can access this new analysis environment. Along with an overall introduction of the environment, the main features of this package are illustrated, and the prospect of using basic WebStat components for more advanced applications is discussed."
The asypow library consists of routines written in the S language that calculate power and related quantities utilizing asymptotic methods. A paper describing these methods with examples is in preparation [1]. Two methods are available. The likelihood ratio method (LR) is described in [2]. Another general method appears recently in [3]; and we designate it the SMO method after the initials of the authors.
"A highly interactive, user-friendly object-oriented software package written in LispStat is introduced that performs simple and multiple correspondence analysis, and profile analysis. These three techniques are integrated into a single environment driven by a user-friendly graphical interface that takes advantage of Lisp-Stat's advanced graphical capabilities. Techniques that assess the stability of the solution are also introduced. Some of the features of the package include colored graphics, incremental graph zooming capabilities, manual point separation to determine identities of overlapping points, and stability and fit measures. The features of the package are used to show some interesting trends in a large educational dataset."
"We consider the fitting of a mixture of two Gompertz distributions to censored survival data.  This model is therefore applicable where there are two distinct causes for failure that act in a mutually exclusive manner, and the baseline failure time for each cause follows a Gompertz distribution.  For example, in a study of a disease such as breast cancer, suppose that failure corresponds to death, whose cause is attributed either to breast cancer or some other cause.  In this example, the mixing proportion for the component of the mixture representing time to death from a cause other than breast cancer may be interpreted to be the cure rate for breast cancer (Gordon, 1990a and 1990b).  This Gompertz mixture model whose components are adjusted multiplicatively to reflect the age of the patient at the origin of the survival time, is fitted by maximum likelihood via the EM algorithm (Dempster, Laird and Rubin, 1977).  There is the provision to handle the case where the mixing proportions are formulated in terms of a logistic model to depend on a vector of covariates associated with each survival time.  The algorithm can also handle the case where there is only one cause of failure, but which may happen at infinity for some patients with a nonzero probability (Farewell, 1982)."
"A computer program for multifactor relative risks, confidence limits, and tests of hypotheses using regression coefficients and a variance-covariance matrix obtained from a previous additive or multiplicative regression analysis is described in detail. Data used by the program can be stored and input from an external disk-file or entered via the keyboard. The output contains a list of the input data, point estimates of single or joint effects, confidence intervals and tests of hypotheses based on a minimum modified chi-square statistic. Availability of the program is also discussed."
"This paper provides a suite of datasets from standard multivariate distributions and simple high-dimensional geomtric shapes that can be used to visually calibrate new users of grand tours. It contains animations of 1-D, 2-D, 3-D, 4-D and 5-D grand tours, links to starting XGobi or XLispStat on the calibration data sets, and C code for generating a grand tour. 

The purpose of the paper is two-fold: providing code for the grand tour that others could pick up and modify (it is not easy to code this version which is why there are very few implementations currently available), and secondly, provide a variety of training datasets to help new users get a visual sense for high-dimensional data."
"The incomplete beta function is defined as where Beta(p, q) is the beta function. Dutka (1981) gave a history of the development and numerical evaluation of this function. In this article, an algorithm for computing first and second derivatives of Ix,p,q with respect to p and q is described. The algorithm is useful, for example, when fitting parameters to a censored beta, truncated beta, or a truncated beta-binomial model."
