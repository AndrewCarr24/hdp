{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import gammaln\n",
    "import data_preproc\n",
    "from data_preproc import data_preproc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "voca, docs = data_preproc(\"tm_test_data.csv\") # load vocab and docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "########### SAMPLING T FUNCTIONS ##########\n",
    "##########################################\n",
    "    \n",
    "def sample_t(j, i, w, doc_j, topic_idx, n_jtw, n_kv, m_k, gamma, alpha, beta, V):\n",
    "    '''For each word in document j (doc_j), sample for posterior distribution of t and update\n",
    "       table and topic assignments, as well as other count structures within jt_info, n_jtw, m_k, and n_kv\n",
    "       Output: updated doc_j, m_k, n_kv, and topic idx (k_idx)'''\n",
    "\n",
    "    tbl = doc_j['t_ji'][i]\n",
    "\n",
    "    ### Remove word if assigned to table (i.e. -x_ji) \n",
    "    if tbl>0:\n",
    "\n",
    "        doc_j, n_jtw, n_kv, m_k, topic_idx = remove_xji(j, doc_j, topic_idx, w, tbl, n_jtw, n_kv, m_k)\n",
    "\n",
    "    #### Sampling t ####\n",
    "    fk = f_k_distribution(w, n_kv)\n",
    "    \n",
    "    # Un-normalized posterior pvals\n",
    "    post_pvals_t = posterior_t(doc_j, fk, m_k, V, alpha, gamma)\n",
    "    post_pvals_t /= post_pvals_t.sum()\n",
    "\n",
    "    post_t_idx = np.random.multinomial(1, post_pvals_t).argmax()\n",
    "\n",
    "    # Get most likely table selection\n",
    "    new_t = doc_j['t_j'][post_t_idx]\n",
    "\n",
    "    # If new table\n",
    "    if new_t == 0:\n",
    "\n",
    "        ### Sampling k when t is NEW ###\n",
    "        post_pvals_kt = posterior_k_new_t(topic_idx, m_k, fk, gamma, V)\n",
    "        post_pvals_kt /= post_pvals_kt.sum()\n",
    "\n",
    "        post_kt_idx = np.random.multinomial(1, post_pvals_kt).argmax()\n",
    "\n",
    "        # Select most likely topic for new table\n",
    "        new_k = topic_idx[post_kt_idx]\n",
    "\n",
    "        ## New topic selected\n",
    "        if new_k == 0:\n",
    "            \n",
    "            # Create new topic\n",
    "            new_k, topic_idx, m_k, n_kv = new_topic(topic_idx, m_k, n_kv, beta, V)\n",
    "\n",
    "        # Add new table\n",
    "        new_t, doc_j, n_jtw = new_table(j, new_k, doc_j, n_jtw)\n",
    "\n",
    "        m_k[new_k] += 1 #add to table cnt for topic k\n",
    "\n",
    "    # Assign word to table\n",
    "    doc_j, n_kv, n_jtw = assign_to_table(j, i, w, new_t, doc_j, n_kv, n_jtw)\n",
    "            \n",
    "    return doc_j, topic_idx, n_jtw, n_kv, m_k   \n",
    "\n",
    "\n",
    "def remove_xji(j, doc_j, topic_idx, w, tbl, n_jtw, n_kv, m_k):\n",
    "    '''Remove word if assigned to table (i.e. -x_ji), calls on remove_table helper function\n",
    "       Inputs: table idx, topic for table t, word\n",
    "       Outputs: updated n_kv, plus additional \n",
    "                 updates on doc_j, m_k (tables in topic k), and k_idx (topics) from remove_table fcn '''\n",
    "    \n",
    "    topic = doc_j['k_jt'][tbl]\n",
    "    \n",
    "    # decrease counts\n",
    "    doc_j['n_jt'][tbl] -=1\n",
    "    n_jtw[j][tbl][w] -=1\n",
    "    n_kv[w, topic] -= 1\n",
    "    \n",
    "    # Empty table, remove it\n",
    "    if doc_j['n_jt'][tbl] == 0:\n",
    "        \n",
    "        doc_j, m_k, topic_idx = remove_table(doc_j, tbl, topic_idx, m_k)\n",
    "        \n",
    "    return doc_j, n_jtw, n_kv, m_k, topic_idx\n",
    "\n",
    "\n",
    "def remove_table(doc_j, tbl, topic_idx, m_k):\n",
    "    '''Empty tables (i.e. n_jt == 0) are removed\n",
    "       Inputs: table idx, doc_j and m_k (tables in topic k)\n",
    "       Outputs: Updated doc_j, m_k, k_idx '''\n",
    "    \n",
    "    # Delete table \n",
    "    doc_j['t_j'].remove(tbl)\n",
    "    topic = doc_j['k_jt'][tbl]\n",
    "    m_k[topic] -=1 # decreate table topic count\n",
    "    \n",
    "    #if no more tables with topic k, remove topic\n",
    "    if m_k[topic] == 0:\n",
    "        topic_idx.remove(topic)\n",
    "        \n",
    "    return doc_j, m_k, topic_idx\n",
    "\n",
    "\n",
    "def f_k_distribution(word,  n_kv):\n",
    "    '''Conditional density of x_ji given k and all data items except x_ji'''\n",
    "    lik = n_kv[word,:] / n_kv.sum(axis=0)\n",
    "    lik[0] = 0\n",
    "    \n",
    "    return lik\n",
    "\n",
    "\n",
    "def posterior_t(doc_j, fk, m_k, V, alpha, gamma):\n",
    "    '''Generate posterior pvals selecting a new or existing table'''\n",
    "    t_j = doc_j['t_j']\n",
    "    \n",
    "    # If t i not new\n",
    "    post_t = doc_j['n_jt'][t_j] * fk[doc_j['k_jt'][t_j]]\n",
    " \n",
    "    # If t is new\n",
    "    post_t_new = np.inner(m_k, fk) + gamma/V\n",
    "    post_t[0] = post_t_new * alpha / (gamma + np.sum(m_k))\n",
    "    \n",
    "    return post_t\n",
    "\n",
    "\n",
    "\n",
    "def posterior_k_new_t(topic_idx, m_k, fk, gamma, V):\n",
    "    '''If new table selected, generate posterior pvals for selecting a new or existing topic'''\n",
    "    post_k = (m_k*fk)[topic_idx] #existing topic\n",
    "    post_k[0] = gamma / V # new topic\n",
    "    \n",
    "    return post_k\n",
    "\n",
    "\n",
    "def new_topic(topic_idx, m_k, n_kv, beta, V):\n",
    "    '''If new topic selected, get new topic k and extend structures k_idx (topic idx), n_kv (word-topic matrix), \n",
    "       m_k (tables per topic) for later updates. \n",
    "       Output: new topic and extended structures'''\n",
    "    \n",
    "    # Search through topic indexes, find correct extension to array given topic might have been removed\n",
    "    for k_idx, k in enumerate(topic_idx):\n",
    "        if k_idx != k:\n",
    "            break\n",
    "    else:\n",
    "        k_idx = len(topic_idx)\n",
    "        if k_idx >= n_kv.shape[1]:\n",
    "            m_k = np.resize(m_k, k_idx + 1)\n",
    "            n_kv = np.c_[n_kv, np.ones((V,1), dtype=int) * beta]\n",
    "\n",
    "    # Add new topic index, set new topic table count to zero and add beta values to new n_kv column\n",
    "    topic_idx.insert(k_idx, k_idx)\n",
    "    m_k[k_idx] = 0\n",
    "    n_kv[:, k_idx] = np.ones(V, dtype=int) * beta\n",
    "    \n",
    "    return k_idx, topic_idx, m_k, n_kv\n",
    "\n",
    "\n",
    "def new_table(j, new_k, doc_j, n_jtw):\n",
    "    '''If new table selected, get new table idx and extend structures doc_j jt_info and n_jtw for \n",
    "       later updates\n",
    "       Output: new table and extended structures'''\n",
    "    \n",
    "    # Search through table indexes, find correct extension to array given tables might have been removed\n",
    "    for t_idx, t in enumerate(doc_j['t_j']):\n",
    "        if t_idx != t:\n",
    "            break\n",
    "    else:\n",
    "        t_idx = len(doc_j['t_j'])\n",
    "        doc_j['n_jt'] = np.resize(doc_j['n_jt'], t_idx + 1)\n",
    "        doc_j['k_jt'] = np.resize(doc_j['k_jt'], t_idx + 1)\n",
    "        n_jtw[j].append(None)\n",
    "            \n",
    "    # Add new table index, set count to zero and initialize word-specific count dictionary\n",
    "    doc_j['t_j'].insert(t_idx, t_idx)\n",
    "    doc_j['n_jt'][t_idx] = 0\n",
    "    n_jtw[j][t_idx] = {}\n",
    "    \n",
    "    doc_j['k_jt'][t_idx] = new_k # set new table topic to (new or old) topic\n",
    "        \n",
    "    return t_idx, doc_j, n_jtw\n",
    "\n",
    "\n",
    "def assign_to_table(j, i, word, new_t, doc_j, n_kv, n_jtw):\n",
    "    '''Assign word to table new_t with topic new_k in doc_j, add counts to overall table count,  \n",
    "       word-topic matrix and discretized table word counts\n",
    "       Outputs: updated doc_j and n_kv'''\n",
    "    \n",
    "    # Get word to table (either new or old) and add to table count\n",
    "    doc_j['t_ji'][i] = new_t\n",
    "    doc_j['n_jt'][new_t] +=1\n",
    "    \n",
    "    # Get topic table\n",
    "    new_k = doc_j['k_jt'][new_t]\n",
    "    \n",
    "    # Add to table word-specific counts and topic-word specific counts\n",
    "    n_kv[word, new_k] += 1\n",
    "    \n",
    "    n_jtw[j][new_t][word] = n_jtw[j][new_t].get(word, 0) + 1\n",
    "    \n",
    "    return doc_j, n_kv, n_jtw\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "########### SAMPLING K FUNCTIONS ##########\n",
    "##########################################\n",
    "\n",
    "def sample_k(j, tbl,  doc_j, topic_idx, n_jtw, n_kv, m_k, beta, V):\n",
    "    '''For each TABLE in document j (doc_j), sample for posterior distribution of k and update\n",
    "       table and topic assignments, as well as other count structures within jt_info, n_jtw, m_k, and n_kv\n",
    "       Output: updated doc_j, m_k, n_kv, and topic idx (k_idx)'''\n",
    "    \n",
    "    #### START of Sampling k loop through tables, (skip first index always, 0 = dummy idx) ####\n",
    "    if tbl != 0:\n",
    "\n",
    "        # Get topic k, remove all components from table t associated with topic k\n",
    "        doc_j, topic_idx, m_k = remove_Xvec_ji(tbl, doc_j, topic_idx, m_k)\n",
    "\n",
    "        # Samples posterior p-vals K\n",
    "        post_pvals_k = posterior_k(j, doc_j, tbl, topic_idx, n_jtw, n_kv, m_k, V, beta)\n",
    "        post_pvals_k /= post_pvals_k.sum()\n",
    "\n",
    "        # Select most likely topic for table\n",
    "        post_k_idx = np.random.multinomial(1, post_pvals_k).argmax()\n",
    "\n",
    "        new_k = topic_idx[post_k_idx]\n",
    "\n",
    "        ## New topic selected\n",
    "        if new_k == 0:\n",
    "\n",
    "            # Create new topic\n",
    "            new_k, topic_idx, m_k, n_kv = new_topic(topic_idx, m_k, n_kv, beta, V)\n",
    "\n",
    "        # Add table to topic k count\n",
    "        m_k[new_k] += 1\n",
    "\n",
    "        # Rearrange individual word-topic counts based on potential new_k reassignment\n",
    "        doc_j, n_kv = rearranging_k_counts(j,tbl, new_k, doc_j, n_jtw, n_kv, beta)\n",
    "\n",
    "    return doc_j, topic_idx, n_kv, m_k\n",
    "\n",
    "\n",
    "def posterior_k(j, doc_j, tbl, topic_idx, n_jtw, n_kv, m_k, V, beta):\n",
    "    '''Compute explicit posterior multinomial-dirichlet posterior distribution'''\n",
    "    \n",
    "    # Topic k of table t\n",
    "    k = doc_j['k_jt'][tbl]\n",
    "    n_jt = doc_j['n_jt'][tbl]\n",
    "    \n",
    "    # Remove all counts associated with topic k in table t, from overall topic counts (n_k)\n",
    "    n_kv = n_kv.copy()\n",
    "    n_k = n_kv.sum(axis = 0)\n",
    "    n_k[k] -= n_jt\n",
    "    n_k = n_k[topic_idx]\n",
    "    \n",
    "    # Initialized k posterior in log-form for simplicity, this computes f_k^{-X_ji} \n",
    "    # has Dirichlet-Multinomial form\n",
    "    log_post_k = np.log(m_k[topic_idx]) + gammaln(n_k) - gammaln(n_k + n_jt)\n",
    "    log_post_k_new = np.log(gamma) + gammaln(V*beta) - gammaln((V*beta) + n_jt)\n",
    "\n",
    "    \n",
    "    # Remove individual word counts associated with topic k\n",
    "    # add their contributions to k posterior\n",
    "    for w_key, w_cnt in n_jtw[j][tbl].items():\n",
    "\n",
    "        if w_cnt == 0: # if word count is 0 skip\n",
    "            continue\n",
    "\n",
    "        # For word w, get counts across topics\n",
    "        w_cnt_k = n_kv[w_key, :]\n",
    "        \n",
    "        # For specific topic k, remove count from associated table t\n",
    "        w_cnt_k[k] -= w_cnt\n",
    "        w_cnt_k = w_cnt_k[topic_idx]\n",
    "        w_cnt_k[0] = 1\n",
    "\n",
    "        # Add contributions of individual observations (words)\n",
    "        log_post_k += gammaln(w_cnt_k  + w_cnt) - gammaln(w_cnt_k)\n",
    "        log_post_k_new += gammaln(beta + w_cnt) - gammaln(beta)\n",
    "    \n",
    "    # p-val for new k\n",
    "    log_post_k[0] = log_post_k_new\n",
    "\n",
    "    # Bring back to non-log realm, normalize k-posterior \n",
    "    post_k = np.exp(log_post_k - log_post_k.max())\n",
    "\n",
    "    return post_k\n",
    "\n",
    "\n",
    "def remove_Xvec_ji(tbl, doc_j, topic_idx, m_k):\n",
    "    '''Remove table from topic k (i.e. related removing all components associated to table t later)\n",
    "       If table becomes empty, remove topic'''\n",
    "    \n",
    "    # Get topic k, remove all components from table t associated with topic k\n",
    "    k_idx = doc_j['k_jt'][tbl]\n",
    " \n",
    "    m_k[k_idx] -=1 # remove from table-topic vector\n",
    "    \n",
    "    # if no more tables with topic k, remove topic k and set table's topic to 0\n",
    "    if m_k[k_idx] == 0:\n",
    "        topic_idx.remove(k_idx)\n",
    "        doc_j['k_jt'][tbl] = 0\n",
    "        \n",
    "    return doc_j, topic_idx, m_k\n",
    "\n",
    "\n",
    "def rearranging_k_counts(j, tbl, new_k, doc_j, n_jtw, n_kv, beta):\n",
    "    '''For sampled k, rearrange counts for topics accordingly (i.e. if a new k was selected, subtract\n",
    "       from previous k and add to new k in word-topic matrix)'''\n",
    "    \n",
    "    k = doc_j['k_jt'][tbl] \n",
    "    \n",
    "    # If new topic for table t is selected, set topic to new topic\n",
    "    if k != new_k:\n",
    "        doc_j['k_jt'][tbl] = new_k\n",
    "        \n",
    "        # On word-topic matrix, move counts from old topic to new topic\n",
    "        for w_key, cnt in n_jtw[j][tbl].items():\n",
    "            if k != 0: \n",
    "                n_kv[w_key, k] -= cnt\n",
    "    \n",
    "            n_kv[w_key, new_k] += cnt\n",
    "            \n",
    "    return doc_j, n_kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "########### HDP ALGORITHM ##########\n",
    "##########################################\n",
    "\n",
    "def run_hdp(docs, voca, gamma, alpha, beta, epochs=1):\n",
    "    ''''''\n",
    "    \n",
    "    #### INITIALIZE PARAMETERS #####\n",
    "    \n",
    "    V = voca.shape[0] # length of vocabulary\n",
    "\n",
    "    D = len(docs) # numb docs\n",
    "    \n",
    "    topic_idx = [0] # topic indexes\n",
    "    \n",
    "    # table specific indexes and counters\n",
    "    doc_arrays = [{'t_j': [0],\n",
    "              'k_jt': np.array([0],dtype=int),\n",
    "              'n_jt': np.array([0],dtype=int),\n",
    "              't_ji': np.zeros(len(docs[j]), dtype=int) - 1 } for j in range(D)]\n",
    "\n",
    "    n_jtw = [[None] for j in range(D)] # word-table specific assignment/counts\n",
    "\n",
    "    # Topic specific counts\n",
    "    m_k = np.ones(1, dtype=int) # tables\n",
    "    n_kv = np.ones((V, 1)) # words-topic count matrix (shape will be V x k)\n",
    "    \n",
    "    x_ji = docs # list of sublists\n",
    "    \n",
    "    ##### INFERENCE LOOPS ######\n",
    "    for z in range(epochs):\n",
    "    \n",
    "        ### Infer t\n",
    "        for j, x_i in enumerate(x_ji):\n",
    "            doc_j = doc_arrays[j]\n",
    "            \n",
    "            for i, w in enumerate(x_i):\n",
    "                doc_j, topic_idx, n_jtw, n_kv, m_k = sample_t(j, i, w, doc_j, topic_idx, n_jtw, n_kv, m_k, gamma, alpha, beta, V)\n",
    "                \n",
    "            doc_arrays[j] = doc_j\n",
    "\n",
    "            \n",
    "        ### Infer k\n",
    "        for j in range(D):\n",
    "            doc_j = doc_arrays[j]\n",
    "            \n",
    "            for tbl in doc_j['t_j']:\n",
    "                doc_j, topic_idx, n_kv, m_k = sample_k(j, tbl, doc_j, topic_idx, n_jtw, n_kv, m_k, beta, V)\n",
    "\n",
    "            doc_arrays[j] = doc_j\n",
    "            \n",
    "\n",
    "    return doc_arrays, topic_idx, n_kv, m_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "#### Initialize params ######\n",
    "#############################\n",
    "\n",
    "# Hyper params\n",
    "beta = 0.5 # word concentration (LDA)\n",
    "alpha = np.random.gamma(1, 1) # GP hyperparam\n",
    "gamma = np.random.gamma(1, 1) # Base GP hyperparam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         2081621 function calls (2031869 primitive calls) in 4.549 seconds\n",
       "\n",
       "   Ordered by: cumulative time\n",
       "   List reduced from 73 to 22 due to restriction <0.3>\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "        1    0.000    0.000    4.549    4.549 {built-in method builtins.exec}\n",
       "        1    0.002    0.002    4.549    4.549 <string>:1(<module>)\n",
       "        1    0.045    0.045    4.546    4.546 <ipython-input-6-e9614e726bdc>:5(run_hdp)\n",
       "    28619    0.256    0.000    3.396    0.000 <ipython-input-5-2f7de8ce6891>:5(sample_t)\n",
       "   226007    1.455    0.000    1.455    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
       "    82111    0.059    0.000    1.146    0.000 {method 'sum' of 'numpy.ndarray' objects}\n",
       "213988/164236    0.167    0.000    1.108    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
       "     8913    0.081    0.000    1.100    0.000 <ipython-input-5-2f7de8ce6891>:201(sample_k)\n",
       "    82111    0.031    0.000    1.087    0.000 _methods.py:36(_sum)\n",
       "    45201    0.456    0.000    0.983    0.000 {method 'multinomial' of 'numpy.random.mtrand.RandomState' objects}\n",
       "    28619    0.096    0.000    0.849    0.000 <ipython-input-5-2f7de8ce6891>:96(f_k_distribution)\n",
       "    28619    0.428    0.000    0.771    0.000 <ipython-input-5-2f7de8ce6891>:104(posterior_t)\n",
       "     8291    0.490    0.000    0.765    0.000 <ipython-input-5-2f7de8ce6891>:236(posterior_k)\n",
       "    90402    0.055    0.000    0.527    0.000 <__array_function__ internals>:2(all)\n",
       "   119021    0.159    0.000    0.513    0.000 fromnumeric.py:73(_wrapreduction)\n",
       "     8291    0.045    0.000    0.420    0.000 <ipython-input-5-2f7de8ce6891>:150(new_table)\n",
       "    90402    0.069    0.000    0.418    0.000 fromnumeric.py:2324(all)\n",
       "    16584    0.011    0.000    0.369    0.000 <__array_function__ internals>:2(resize)\n",
       "    16584    0.103    0.000    0.344    0.000 fromnumeric.py:1348(resize)\n",
       "    28619    0.020    0.000    0.257    0.000 <__array_function__ internals>:2(sum)\n",
       "    28619    0.045    0.000    0.219    0.000 fromnumeric.py:2092(sum)\n",
       "    28619    0.084    0.000    0.090    0.000 <ipython-input-5-2f7de8ce6891>:175(assign_to_table)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%prun -l 0.30 -s cumtime run_hdp(docs, voca, gamma, alpha, beta, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
